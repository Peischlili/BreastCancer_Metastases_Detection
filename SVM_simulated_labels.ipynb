{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore ResNet feature matrices\n",
    "image_folder = \"train_input/resnet_features/\"\n",
    "#image = np.load('/tmp/123.npy', mmap_mode='r')\n",
    "\n",
    "# Function to load folder into arrays and then it returns that same array\n",
    "def loadImages(path):\n",
    "    image_files = sorted([os.path.join(path, file)\n",
    "         for file in os.listdir(path) if file.endswith('.npy')])\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_features(filenames):\n",
    "    \"\"\"Load and aggregate the resnet features by the average.\n",
    "\n",
    "    Args:\n",
    "        filenames: list of filenames of length `num_patients` corresponding to resnet features\n",
    "\n",
    "    Returns:\n",
    "        features: np.array of mean resnet features, shape `(num_patients, 2048)`\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    features = []\n",
    "    for f in filenames:\n",
    "        patient_features = np.load(f)\n",
    "\n",
    "        # Remove location features (but we could use them?)\n",
    "        patient_features = patient_features[:, 3:]\n",
    "\n",
    "        aggregated_features = np.mean(patient_features, axis=0)\n",
    "        features.append(aggregated_features)\n",
    "\n",
    "    features = np.stack(features, axis=0)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature npy folder into arrays and then it returns that same array of strings\n",
    "def loadFiles(path):\n",
    "    feature_files = sorted([os.path.join(path, file)\n",
    "         for file in os.listdir(path) if file.endswith('.npy')])\n",
    "    return feature_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precise training set and test set relative location\n",
    "train_dir = Path(\"train_input/resnet_features\")\n",
    "test_dir = Path(\"test_input/resnet_features\")\n",
    "\n",
    "train_output_filename = Path(\"training_output.csv\")\n",
    "\n",
    "train_output = pd.read_csv(train_output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use locally annotated information</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old id looks like: ID_387_annotated_tile_0_15_69_30.jpg\n",
    "# reformat the annotation id for upcoming df joining\n",
    "# function for label file\n",
    "\n",
    "def get_new_id(old_id):\n",
    "    # get rid of the .jpg extension\n",
    "    old_id = Path(old_id).stem\n",
    "    # store all characters in a list\n",
    "    string_list = old_id.split('_')\n",
    "    # new_id looks like: <patient_id>_<zoom_level>_<x_coord>_<y_coord>\n",
    "    new_id = f\"{string_list[0]}_{string_list[1]}_{string_list[-3]}_{string_list[-2]}_{string_list[-1]}\"\n",
    "    return new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load local annotations (tile-level)\n",
    "# id goes as follows: <patient_id>_annotated_tile_<tile_id>_<tile_coords>\n",
    "local_annot = pd.read_csv('train_input/train_tile_annotations.csv')\n",
    "local_annot.rename(columns={'Unnamed: 0': 'Tile_annotation_id'}, inplace=True)\n",
    "\n",
    "# add new column new_id\n",
    "local_annot['new_tile_id'] = local_annot['Tile_annotation_id'].map(get_new_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tile_annotation_id</th>\n",
       "      <th>Target</th>\n",
       "      <th>new_tile_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_387_annotated_tile_0_15_69_30.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_387_15_69_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_387_annotated_tile_1_15_23_53.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_387_15_23_53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_387_annotated_tile_2_15_58_20.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_387_15_58_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_387_annotated_tile_3_15_67_12.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_387_15_67_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_387_annotated_tile_4_15_57_20.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_387_15_57_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10119</th>\n",
       "      <td>ID_035_annotated_tile_861_16_73_121.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID_035_16_73_121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10120</th>\n",
       "      <td>ID_035_annotated_tile_862_16_67_126.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID_035_16_67_126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>ID_035_annotated_tile_863_16_24_116.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_035_16_24_116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>ID_035_annotated_tile_864_16_69_119.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ID_035_16_69_119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>ID_035_annotated_tile_865_16_56_120.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ID_035_16_56_120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10124 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Tile_annotation_id  Target       new_tile_id\n",
       "0         ID_387_annotated_tile_0_15_69_30.jpg     0.0   ID_387_15_69_30\n",
       "1         ID_387_annotated_tile_1_15_23_53.jpg     0.0   ID_387_15_23_53\n",
       "2         ID_387_annotated_tile_2_15_58_20.jpg     0.0   ID_387_15_58_20\n",
       "3         ID_387_annotated_tile_3_15_67_12.jpg     0.0   ID_387_15_67_12\n",
       "4         ID_387_annotated_tile_4_15_57_20.jpg     0.0   ID_387_15_57_20\n",
       "...                                        ...     ...               ...\n",
       "10119  ID_035_annotated_tile_861_16_73_121.jpg     1.0  ID_035_16_73_121\n",
       "10120  ID_035_annotated_tile_862_16_67_126.jpg     1.0  ID_035_16_67_126\n",
       "10121  ID_035_annotated_tile_863_16_24_116.jpg     0.0  ID_035_16_24_116\n",
       "10122  ID_035_annotated_tile_864_16_69_119.jpg     0.0  ID_035_16_69_119\n",
       "10123  ID_035_annotated_tile_865_16_56_120.jpg     1.0  ID_035_16_56_120\n",
       "\n",
       "[10124 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_annot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load 11 annotated npy files in resnet features folder\n",
    "def loadAnnotatedData(path):\n",
    "    feature_files = sorted([os.path.join(path, file)\n",
    "         for file in os.listdir(path) if file.endswith('_annotated.npy')])\n",
    "    return feature_files\n",
    "\n",
    "\n",
    "# Compile all data into a dataframe to form strong supervised dataset (local data)\n",
    "def dataCompiler(filelist):\n",
    "    df_data = pd.DataFrame()\n",
    "    \n",
    "    # Load numpy arrays\n",
    "    for f in filelist:\n",
    "        try:\n",
    "            patient_features = np.load(f)\n",
    "            patient_id = Path(f).stem.strip(\"_annotated\")\n",
    "\n",
    "            # add patient id to features\n",
    "            df_patient = pd.DataFrame(data=patient_features)\n",
    "            df_patient['patient_id'] = patient_id\n",
    "\n",
    "            # add df to global dataframe\n",
    "            df_data = df_data.append(df_patient, ignore_index=True)\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"{f} does not exist.\")\n",
    "    \n",
    "    # rename dataframe with proper column names\n",
    "    colnames = ['zoom_level', 'x_coord', 'y_coord'] + [i for i in range(1,2049)] + ['patient_id']\n",
    "    df_data.columns = colnames\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "\n",
    "# generate new_id for annotated data\n",
    "# funciton for 2048-dimension feature file\n",
    "\n",
    "def generate_new_id(patient_id, zoom, x, y):\n",
    "    element_list = [patient_id, str(int(zoom)), str(int(x)), str(int(y))]\n",
    "    separator = \"_\"\n",
    "    new_id = separator.join(element_list)\n",
    "    return new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the list of annotated patient npy files\n",
    "annotatedFiles_train = loadAnnotatedData(train_dir)\n",
    "\n",
    "# Complete annotated dataset\n",
    "annotatedData = dataCompiler(annotatedFiles_train)\n",
    "\n",
    "# add new column new_tile_id\n",
    "annotatedData['new_tile_id']:str = annotatedData.apply(lambda x: generate_new_id(x.patient_id, x.zoom_level, x.x_coord, x.y_coord),  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zoom_level</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "      <th>2048</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>new_tile_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.063937</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117584</td>\n",
       "      <td>0.005884</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.319746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089043</td>\n",
       "      <td>ID_035</td>\n",
       "      <td>ID_035_16_56_117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.946910</td>\n",
       "      <td>2.319555</td>\n",
       "      <td>0.036641</td>\n",
       "      <td>0.007913</td>\n",
       "      <td>0.148661</td>\n",
       "      <td>0.071836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.497255</td>\n",
       "      <td>0.085011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395565</td>\n",
       "      <td>0.758223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ID_035</td>\n",
       "      <td>ID_035_16_47_136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.393728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041272</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.129262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015349</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ID_035</td>\n",
       "      <td>ID_035_16_38_117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.231148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015671</td>\n",
       "      <td>0.326770</td>\n",
       "      <td>0.345363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ID_035</td>\n",
       "      <td>ID_035_16_40_128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.290326</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.032283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115174</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>0.107415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046535</td>\n",
       "      <td>ID_035</td>\n",
       "      <td>ID_035_16_47_117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10119</th>\n",
       "      <td>15.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.874400</td>\n",
       "      <td>0.044763</td>\n",
       "      <td>0.413319</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>0.549523</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296946</td>\n",
       "      <td>0.037002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076320</td>\n",
       "      <td>ID_387</td>\n",
       "      <td>ID_387_15_34_42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10120</th>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.092193</td>\n",
       "      <td>0.591326</td>\n",
       "      <td>0.217773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>0.025986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.629614</td>\n",
       "      <td>0.506654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ID_387</td>\n",
       "      <td>ID_387_15_54_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>15.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.788472</td>\n",
       "      <td>2.213521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022404</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>0.449662</td>\n",
       "      <td>0.063583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018248</td>\n",
       "      <td>0.052106</td>\n",
       "      <td>0.023716</td>\n",
       "      <td>0.324964</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>1.230977</td>\n",
       "      <td>1.443332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ID_387</td>\n",
       "      <td>ID_387_15_78_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>15.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.728631</td>\n",
       "      <td>1.686132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061719</td>\n",
       "      <td>0.130151</td>\n",
       "      <td>0.286733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>0.095885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411714</td>\n",
       "      <td>1.435175</td>\n",
       "      <td>0.483119</td>\n",
       "      <td>0.010892</td>\n",
       "      <td>ID_387</td>\n",
       "      <td>ID_387_15_67_33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>15.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.141584</td>\n",
       "      <td>0.022645</td>\n",
       "      <td>0.657994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.043770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367387</td>\n",
       "      <td>0.056908</td>\n",
       "      <td>0.027838</td>\n",
       "      <td>ID_387</td>\n",
       "      <td>ID_387_15_37_73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10124 rows × 2053 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       zoom_level  x_coord  y_coord         1         2         3         4  \\\n",
       "0            16.0     56.0    117.0  0.063937  0.000000  0.117584  0.005884   \n",
       "1            16.0     47.0    136.0  0.946910  2.319555  0.036641  0.007913   \n",
       "2            16.0     38.0    117.0  0.393728  0.000000  0.031280  0.000000   \n",
       "3            16.0     40.0    128.0  0.231148  0.000000  0.000126  0.000000   \n",
       "4            16.0     47.0    117.0  0.290326  0.002089  0.032283  0.000000   \n",
       "...           ...      ...      ...       ...       ...       ...       ...   \n",
       "10119        15.0     34.0     42.0  0.874400  0.044763  0.413319  0.013475   \n",
       "10120        15.0     54.0     20.0  0.092193  0.591326  0.217773  0.000000   \n",
       "10121        15.0     78.0     20.0  0.788472  2.213521  0.000000  0.022404   \n",
       "10122        15.0     67.0     33.0  0.728631  1.686132  0.000000  0.061719   \n",
       "10123        15.0     37.0     73.0  0.141584  0.022645  0.657994  0.000000   \n",
       "\n",
       "              5         6         7  ...      2041      2042      2043  \\\n",
       "0      0.271771  0.007639  0.000000  ...  0.000000  0.010490  0.000000   \n",
       "1      0.148661  0.071836  0.000000  ...  0.022700  0.497255  0.085011   \n",
       "2      0.041272  0.005072  0.129262  ...  0.000000  0.000000  0.000000   \n",
       "3      0.015671  0.326770  0.345363  ...  0.000000  0.027319  0.000000   \n",
       "4      0.115174  0.012818  0.107415  ...  0.000000  0.042446  0.000000   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "10119  0.549523  0.565158  0.000000  ...  0.296946  0.037002  0.000000   \n",
       "10120  0.068469  0.000000  0.000000  ...  0.003810  0.025986  0.000000   \n",
       "10121  0.032162  0.449662  0.063583  ...  0.018248  0.052106  0.023716   \n",
       "10122  0.130151  0.286733  0.000000  ...  0.009419  0.095885  0.000000   \n",
       "10123  0.002757  0.000000  0.000000  ...  0.000040  0.043770  0.000000   \n",
       "\n",
       "           2044      2045      2046      2047      2048  patient_id  \\\n",
       "0      0.026191  0.000000  0.319746  0.000000  0.089043      ID_035   \n",
       "1      0.000000  0.000000  0.395565  0.758223  0.000000      ID_035   \n",
       "2      0.000000  0.000000  0.015349  0.000000  0.000000      ID_035   \n",
       "3      0.000000  0.000000  0.007136  0.000000  0.000000      ID_035   \n",
       "4      0.000000  0.000000  0.034594  0.000000  0.046535      ID_035   \n",
       "...         ...       ...       ...       ...       ...         ...   \n",
       "10119  0.125798  0.000000  0.000000  0.000000  0.076320      ID_387   \n",
       "10120  0.000000  0.000000  1.629614  0.506654  0.000000      ID_387   \n",
       "10121  0.324964  0.008178  1.230977  1.443332  0.000000      ID_387   \n",
       "10122  0.000000  0.411714  1.435175  0.483119  0.010892      ID_387   \n",
       "10123  0.000000  0.000000  0.367387  0.056908  0.027838      ID_387   \n",
       "\n",
       "            new_tile_id  \n",
       "0      ID_035_16_56_117  \n",
       "1      ID_035_16_47_136  \n",
       "2      ID_035_16_38_117  \n",
       "3      ID_035_16_40_128  \n",
       "4      ID_035_16_47_117  \n",
       "...                 ...  \n",
       "10119   ID_387_15_34_42  \n",
       "10120   ID_387_15_54_20  \n",
       "10121   ID_387_15_78_20  \n",
       "10122   ID_387_15_67_33  \n",
       "10123   ID_387_15_37_73  \n",
       "\n",
       "[10124 rows x 2053 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotatedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold\">Join the features table and label table with newly created new_tile_id</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = annotatedData.merge(local_annot, how='inner', on='new_tile_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold\">Separate now local features and target for models</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local features for train (tile-level)\n",
    "#cols = [i for i in range(1, 2049)]\n",
    "local_features = np.array(data.iloc[:, 3:2051])\n",
    "\n",
    "# local targets (tile-level)\n",
    "local_labels = data[\"Target\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Apply SVM to local-annotated data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get filenames for train\n",
    "filenames_train = loadFiles(train_dir)\n",
    "\n",
    "# Get global labels (patient-wise) for train\n",
    "labels_train = train_output[\"Target\"].values\n",
    "\n",
    "# check if the number of observations and labels corresponds\n",
    "assert len(filenames_train) == len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numpy filenames for test\n",
    "filenames_test = loadFiles(test_dir)\n",
    "# ID list without its suffix (ex: \"ID_005\")\n",
    "ids_test = [Path(f).stem for f in filenames_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the resnet features and aggregate them by the average\n",
    "features_train = get_average_features(filenames_train)\n",
    "features_test = get_average_features(filenames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given path of a filename, returns a numpy array\n",
    "def get_tile_features(filename):\n",
    "    # Load npy to numpy arrays \n",
    "    patient_features = np.load(filename)\n",
    "    \n",
    "    # Remove location features (but we could use them?)\n",
    "    patient_features = patient_features[:, 3:]\n",
    "    return patient_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale training set and test set together\n",
    "# can normally have better performance\n",
    "\n",
    "# Standardize training features and apply standardization to test features\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(local_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting strong labels by tile-level resnet\n",
      "AUC: mean 0.919714839559539, std 0.01773988538222485\n",
      "Predicting strong labels by tile-level resnet\n",
      "Accuracy: mean 0.9578557068267216, std 0.004225995867068012\n",
      "Predicting strong labels by tile-level resnet\n",
      "True Positive Rate: mean 0.714234342223554, std 0.044187437945193704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Use the tile-level resnet features to predict the labels\n",
    "\n",
    "# number of runs for cross-validation\n",
    "num_runs = 3\n",
    "# number of splits for cross-validation\n",
    "num_splits = 5\n",
    "\n",
    "# Multiple cross validations on the local feature training set\n",
    "aucs = []\n",
    "accuracies =[]\n",
    "recalls = []\n",
    "\n",
    "for seed in range(num_runs):\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(kernel='linear') # Linear Kernel without C parameter\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Cross validation on the training set\n",
    "    auc = sklearn.model_selection.cross_val_score(clf, X=train_X, y=local_labels,\n",
    "                                                  cv=cv, scoring=\"roc_auc\", verbose=1)\n",
    "    accuracy = sklearn.model_selection.cross_val_score(clf, X=train_X, y=local_labels,\n",
    "                                                  cv=cv, scoring=\"accuracy\", verbose=1)\n",
    "    recall = sklearn.model_selection.cross_val_score(clf, X=train_X, y=local_labels,\n",
    "                                                  cv=cv, scoring=\"recall\", verbose=1)\n",
    "    \n",
    "    aucs.append(auc)\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "\n",
    "aucs = np.array(aucs)\n",
    "accuracies = np.array(accuracies)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "print(\"Predicting strong labels by tile-level resnet\")\n",
    "print(\"AUC: mean {}, std {}\".format(aucs.mean(), aucs.std()))\n",
    "\n",
    "print(\"Predicting strong labels by tile-level resnet\")\n",
    "print(\"Accuracy: mean {}, std {}\".format(accuracies.mean(), accuracies.std()))\n",
    "\n",
    "print(\"Predicting strong labels by tile-level resnet\")\n",
    "print(\"True Positive Rate: mean {}, std {}\".format(recalls.mean(), recalls.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 0.008\n",
      "AUC: mean 0.9409161675829241, std 0.009161927498755718\n",
      "Accuracy: mean 0.9711573220123946, std 0.005224424078312489\n",
      "True Positive Rate: mean 0.7156128258915194, std 0.04897567906589284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 0.04\n",
      "AUC: mean 0.9227796457698807, std 0.018378565926560293\n",
      "Accuracy: mean 0.9599958522422289, std 0.006192905181682128\n",
      "True Positive Rate: mean 0.7198281889921087, std 0.06031469059858214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 0.2\n",
      "AUC: mean 0.9185350830270054, std 0.02124387175568656\n",
      "Accuracy: mean 0.9584154101400477, std 0.0062590471388418055\n",
      "True Positive Rate: mean 0.7184197382878833, std 0.0665063473717086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 1.0\n",
      "AUC: mean 0.9185350830270054, std 0.02124387175568656\n",
      "Accuracy: mean 0.9584154101400477, std 0.0062590471388418055\n",
      "True Positive Rate: mean 0.7184197382878833, std 0.0665063473717086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 5.0\n",
      "AUC: mean 0.9185350830270054, std 0.02124387175568656\n",
      "Accuracy: mean 0.9584154101400477, std 0.0062590471388418055\n",
      "True Positive Rate: mean 0.7184197382878833, std 0.0665063473717086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 25.0\n",
      "AUC: mean 0.9185350830270054, std 0.02124387175568656\n",
      "Accuracy: mean 0.9584154101400477, std 0.0062590471388418055\n",
      "True Positive Rate: mean 0.7184197382878833, std 0.0665063473717086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c_coeff is 125.0\n",
      "AUC: mean 0.9185350830270054, std 0.02124387175568656\n",
      "Accuracy: mean 0.9584154101400477, std 0.0062590471388418055\n",
      "True Positive Rate: mean 0.7184197382878833, std 0.0665063473717086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Test using the following values for coefficient 'c'\n",
    "c_coeff = np.array([5**-3, 5**-2, 5**-1, 1 , 5 , 5**2, 5**3])\n",
    "\n",
    "# number of runs for cross-validation\n",
    "num_runs = 3\n",
    "# number of splits for cross-validation\n",
    "num_splits = 5\n",
    "\n",
    "# Multiple cross validations on the local feature training set\n",
    "aucs = []\n",
    "accuracies =[]\n",
    "recalls = []\n",
    "\n",
    "for i in c_coeff:\n",
    "    # use the C parameter of SVM\n",
    "    svf = SVC(C=i, kernel='linear')\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Cross validation on the training set\n",
    "    auc = sklearn.model_selection.cross_val_score(svf, X=train_X, y=local_labels,\n",
    "                                              cv=cv, scoring=\"roc_auc\", verbose=1)\n",
    "    accuracy = sklearn.model_selection.cross_val_score(svf, X=train_X, y=local_labels,\n",
    "                                              cv=cv, scoring=\"accuracy\", verbose=1)\n",
    "    recall = sklearn.model_selection.cross_val_score(svf, X=train_X, y=local_labels,\n",
    "                                              cv=cv, scoring=\"recall\", verbose=1)\n",
    "\n",
    "    print(f\"When c_coeff is {i}\")\n",
    "    print(\"AUC: mean {}, std {}\".format(auc.mean(), auc.std()))\n",
    "    print(\"Accuracy: mean {}, std {}\".format(accuracy.mean(), accuracy.std()))\n",
    "    print(\"True Positive Rate: mean {}, std {}\".format(recall.mean(), recall.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Grid Search for SVM with RBF Kernel</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is usually a good idea to scale the data for SVM training.\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(local_features)\n",
    "\n",
    "# Train classifiers\n",
    "# For an initial search, a logarithmic grid with basis\n",
    "# 10 is often helpful. Using a basis of 2, a finer\n",
    "# tuning can be achieved but at a much higher cost.\n",
    "\n",
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "grid = sklearn.model_selection.GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, local_labels)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Finally the grid search of RBF takes too long to find the optimal hyperparameters. Since the data dimension is relatively high, theoractically there is no need for data projection and the linear kernel can reach already a comparable performance. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Annotate unlabeled positive cases</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve all positive patient case\n",
    "positive_patients = train_output[train_output['Target']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-93-508d2f94b868>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  positive_patients['npy_ID'] = positive_patients.apply(lambda x: f\"ID_{str(x['ID']).zfill(3)}.npy\", axis=1)\n"
     ]
    }
   ],
   "source": [
    "positive_patients['npy_ID'] = positive_patients.apply(lambda x: f\"ID_{str(x['ID']).zfill(3)}.npy\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all positive case npys to an array for compilation\n",
    "added_pat_list = np.array(positive_patients['npy_ID'])\n",
    "added_pat_list = [f\"train_input/resnet_features/{i}\" for i in added_pat_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input/resnet_features/ID_035.npy does not exist.\n",
      "train_input/resnet_features/ID_036.npy does not exist.\n",
      "train_input/resnet_features/ID_041.npy does not exist.\n",
      "train_input/resnet_features/ID_046.npy does not exist.\n",
      "train_input/resnet_features/ID_129.npy does not exist.\n",
      "train_input/resnet_features/ID_166.npy does not exist.\n",
      "train_input/resnet_features/ID_174.npy does not exist.\n",
      "train_input/resnet_features/ID_218.npy does not exist.\n",
      "train_input/resnet_features/ID_243.npy does not exist.\n",
      "train_input/resnet_features/ID_262.npy does not exist.\n",
      "train_input/resnet_features/ID_387.npy does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Compile unlabeled positive cases in training set\n",
    "posData = dataCompiler(added_pat_list)\n",
    "\n",
    "# add new column new_tile_id\n",
    "posData['new_tile_id']:str = posData.apply(lambda x: generate_new_id(x.patient_id, x.zoom_level, x.x_coord, x.y_coord),  axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training test set for prediction\n",
    "added_features = np.array(posData.iloc[:, 3:2051])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train SVM model with tuned hyperparameter and predict on the added unlabeled set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize training and test features and apply standardization to test features\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(local_features)\n",
    "test_X = scaler.transform(added_features)\n",
    "\n",
    "# Use the tile-level resnet features to predict the labels\n",
    "# Create a svm Classifier\n",
    "clf = svm.SVC(C=0.008, kernel='linear', probability=True)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(train_X, local_labels)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(test_X)\n",
    "y_pred_proba = clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recap prediction info to dataframe\n",
    "recap = pd.DataFrame(data=y_pred_proba)\n",
    "recap['Target'] = y_pred\n",
    "recap['patient_id'] = posData['patient_id']\n",
    "recap.columns = ['negative', 'positive', 'Target', 'ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NB_neg</th>\n",
       "      <th>NB_pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID_008</th>\n",
       "      <td>985.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_010</th>\n",
       "      <td>437.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_013</th>\n",
       "      <td>985.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_018</th>\n",
       "      <td>984.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_019</th>\n",
       "      <td>835.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_375</th>\n",
       "      <td>720.0</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_377</th>\n",
       "      <td>948.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_378</th>\n",
       "      <td>712.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_390</th>\n",
       "      <td>993.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_393</th>\n",
       "      <td>433.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        NB_neg  NB_pos\n",
       "ID                    \n",
       "ID_008   985.0    15.0\n",
       "ID_010   437.0     5.0\n",
       "ID_013   985.0    15.0\n",
       "ID_018   984.0    16.0\n",
       "ID_019   835.0    10.0\n",
       "...        ...     ...\n",
       "ID_375   720.0   280.0\n",
       "ID_377   948.0    52.0\n",
       "ID_378   712.0   288.0\n",
       "ID_390   993.0     7.0\n",
       "ID_393   433.0     7.0\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recap_count = pd.pivot_table(recap, index=['ID'], columns=['Target'], aggfunc={'Target': 'count'})\n",
    "recap_count.columns = ['NB_neg', 'NB_pos']\n",
    "recap_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# check if patient-level data to be eliminated\n",
    "eliminated = recap_count[recap_count['NB_pos'].isnull()]\n",
    "print(len(eliminated)) # all global prediction points to positive, fitting our expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to eliminated\n",
    "eliminated.reset_index(inplace=True)\n",
    "eli_list = list(eliminated['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do top scoring example selections \n",
    "# form a simulated dataset to complete the old locally annotated one \n",
    "posData['Target'] = y_pred\n",
    "posData['positive_proba'] = recap['positive']\n",
    "neg_examples = posData[posData['positive_proba']<0.003]\n",
    "pos_examples = posData[posData['positive_proba']>=0.95]\n",
    "selected_examples = pd.concat([neg_examples, pos_examples], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate those samples in the list from the data set\n",
    "selected_examples = selected_examples[~selected_examples['patient_id'].isin(eli_list)]\n",
    "\n",
    "# form simulated dataset from unlabeled data\n",
    "simulated_features = np.array(selected_examples.iloc[:, 3:2051])\n",
    "simulated_labels = selected_examples['Target'].values\n",
    "\n",
    "# add the simulated features and labels to labelised training data\n",
    "# to form a bigger training set\n",
    "new_features = np.append(local_features, simulated_features, axis=0)\n",
    "new_labels = np.append(local_labels, simulated_labels, axis=0)\n",
    "assert new_features.shape[0] == new_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Take newly-formed dataset as input and predict on the test set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 35.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 34.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting strong labels by tile-level resnet\n",
      "AUC: mean 0.9928278659609465, std 0.0017427640496536488\n",
      "Accuracy: mean 0.9920213094740761, std 0.0008569568035782643\n",
      "True Positive Rate: mean 0.9102156068271996, std 0.01645458678175664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 33.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "# number of runs for cross-validation\n",
    "num_runs = 3\n",
    "# number of splits for cross-validation\n",
    "num_splits = 5\n",
    "\n",
    "# Multiple cross validations on the local feature training set\n",
    "aucs = []\n",
    "accuracies =[]\n",
    "recalls = []\n",
    "\n",
    "# Standardize training and test features and apply standardization to test features\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(new_features)\n",
    "\n",
    "for seed in range(num_runs):\n",
    "    #Create a svm Classifier\n",
    "    clf = svm.SVC(C=0.008, kernel='linear', probability=True) # Linear Kernel without C parameter\n",
    "\n",
    "    cv = sklearn.model_selection.StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Cross validation on the training set\n",
    "    auc = sklearn.model_selection.cross_val_score(clf, X=train_X, y=new_labels,\n",
    "                                                  cv=cv, scoring=\"roc_auc\", verbose=1)\n",
    "    accuracy = sklearn.model_selection.cross_val_score(clf, X=train_X, y=new_labels,\n",
    "                                                  cv=cv, scoring=\"accuracy\", verbose=1)\n",
    "    recall = sklearn.model_selection.cross_val_score(clf, X=train_X, y=new_labels,\n",
    "                                                  cv=cv, scoring=\"recall\", verbose=1)\n",
    "    \n",
    "    aucs.append(auc)\n",
    "    accuracies.append(accuracy)\n",
    "    recalls.append(recall)\n",
    "\n",
    "aucs = np.array(aucs)\n",
    "accuracies = np.array(accuracies)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "print(\"Predicting strong labels by tile-level resnet\")\n",
    "print(\"AUC: mean {}, std {}\".format(aucs.mean(), aucs.std()))\n",
    "print(\"Accuracy: mean {}, std {}\".format(accuracies.mean(), accuracies.std()))\n",
    "print(\"True Positive Rate: mean {}, std {}\".format(recalls.mean(), recalls.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty df to store output of all patients\n",
    "df_output = pd.DataFrame()\n",
    "\n",
    "# Standardize training and test features and apply standardization to test features\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(new_features)\n",
    "\n",
    "# Use the tile-level resnet features to predict the labels\n",
    "# Create a svm Classifier\n",
    "clf = svm.SVC(C=0.008, kernel='linear', probability=True)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(train_X, new_labels)\n",
    "\n",
    "for f in filenames_test: \n",
    "    patient_id:str = Path(f).stem.split(\"ID_\")[1]\n",
    "    test_X = get_tile_features(f)\n",
    "    \n",
    "    # do the PCA transformation on test set\n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    # Predict the response for test dataset\n",
    "    y_pred = clf.predict(test_X)\n",
    "    y_pred_proba = clf.predict_proba(test_X)[:, 1] #keep only positive probability\n",
    "    \n",
    "    # Check that predictions are in [0, 1]\n",
    "    assert np.max(y_pred_proba) <= 1.0\n",
    "    assert np.min(y_pred_proba) >= 0.0\n",
    "    \n",
    "    test_output = pd.DataFrame({\"ID\": patient_id, \"Target\": y_pred_proba, \"Category\": y_pred})\n",
    "    df_output = df_output.append(test_output, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap = pd.pivot_table(df_output, values='Target', index=['ID'], columns=['Category'], aggfunc={'Target': np.mean})\n",
    "# rename recap table\n",
    "colnames = ['negative_proba', 'positive_proba']\n",
    "recap.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if positive proba is absent, ie. no positive class detected for a patient\n",
    "# choose negative proba. Otherwise, use positive proba\n",
    "def select_final_proba(pos_proba, nega_proba):\n",
    "    if np.isnan(pos_proba):\n",
    "        final_proba = nega_proba\n",
    "    else:\n",
    "        final_proba = pos_proba\n",
    "    return final_proba\n",
    "\n",
    "# apply the function to new column of dataframe\n",
    "recap['Target'] = recap.apply(lambda x: select_final_proba(x.positive_proba, x.negative_proba), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns and save result to csv\n",
    "output = recap.drop(columns=['negative_proba', 'positive_proba'])\n",
    "output.to_csv(\"predictions/SVM_with_simulated_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Try without simulated data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty df to store output of all patients\n",
    "df_output = pd.DataFrame()\n",
    "\n",
    "# Standardize training and test features and apply standardization to test features\n",
    "scaler = StandardScaler()\n",
    "train_X = scaler.fit_transform(local_features)\n",
    "\n",
    "# Use the tile-level resnet features to predict the labels\n",
    "# Create a svm Classifier\n",
    "clf = svm.SVC(C=0.008, kernel='linear', probability=True)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(train_X, local_labels)\n",
    "\n",
    "for f in filenames_test: \n",
    "    patient_id:str = Path(f).stem.split(\"ID_\")[1]\n",
    "    test_X = get_tile_features(f)\n",
    "    \n",
    "    # do the PCA transformation on test set\n",
    "    test_X = scaler.transform(test_X)\n",
    "    \n",
    "    # Predict the response for test dataset\n",
    "    y_pred = clf.predict(test_X)\n",
    "    y_pred_proba = clf.predict_proba(test_X)[:, 1] #keep only positive probability\n",
    "    \n",
    "    # Check that predictions are in [0, 1]\n",
    "    assert np.max(y_pred_proba) <= 1.0\n",
    "    assert np.min(y_pred_proba) >= 0.0\n",
    "    \n",
    "    test_output = pd.DataFrame({\"ID\": patient_id, \"Target\": y_pred_proba, \"Category\": y_pred})\n",
    "    df_output = df_output.append(test_output, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap = pd.pivot_table(df_output, values='Target', index=['ID'], columns=['Category'], aggfunc={'Target': np.mean})\n",
    "# rename recap table\n",
    "colnames = ['negative_proba', 'positive_proba']\n",
    "recap.columns = colnames\n",
    "\n",
    "# apply the function to new column of dataframe\n",
    "recap['Target'] = recap.apply(lambda x: select_final_proba(x.positive_proba, x.negative_proba), axis=1)\n",
    "\n",
    "# drop useless columns and save result to csv\n",
    "output = recap.drop(columns=['negative_proba', 'positive_proba'])\n",
    "output.to_csv(\"predictions/SVM.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Try to add accuracy control on output</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redo_category(category, accur_rate):\n",
    "    if category == 1:\n",
    "        draw = np.random.uniform(0.0, 1.0, 1)[0]\n",
    "        # if randomly generated draw is greater than the recall_ration\n",
    "        # then its category is changed to 0 (negative)\n",
    "        if draw > accur_rate:\n",
    "            new_category = 0\n",
    "        else: \n",
    "            new_category = 1\n",
    "    else:\n",
    "        new_category = 0\n",
    "    return new_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column to df that takes into consideration the TPR\n",
    "accur = 0.9712\n",
    "df_output['New_Category'] = df_output.apply(lambda x: redo_category(x.Category, accur), axis = 1)\n",
    "recap = pd.pivot_table(df_output, values='Target', index=['ID'], columns=['Category'], aggfunc={'Target': np.mean})\n",
    "\n",
    "# rename recap table\n",
    "colnames = ['negative_proba', 'positive_proba']\n",
    "recap.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if positive proba is absent, ie. no positive class detected for a patient\n",
    "# choose negative proba. Otherwise, use positive proba\n",
    "def select_final_proba(pos_proba, nega_proba):\n",
    "    if np.isnan(pos_proba):\n",
    "        final_proba = nega_proba\n",
    "    else:\n",
    "        final_proba = pos_proba\n",
    "    return final_proba\n",
    "\n",
    "# apply the function to new column of dataframe\n",
    "recap['Target'] = recap.apply(lambda x: select_final_proba(x.positive_proba, x.negative_proba), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns and save result to csv\n",
    "output = recap.drop(columns=['negative_proba', 'positive_proba'])\n",
    "output.to_csv(\"predictions/SVM_with_accuracy_control.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
